Running:

/usr/local/hadoop-1.0.1/bin/hadoop jar ./tmp_jars/repast.simphony.hadoop.jar repast.simphony.engine.BatchJob -files test_data/batch_params.xml  -libjars tmp_jars/repast.simphony.batch.jar,tmp_jars/repast.simphony.core.jar,tmp_jars/collections-generic-4.01.jar,tmp_jars/saf.core.runtime.jar,tmp_jars/jscience.jar hadoop_input.txt ./output

Deleting the output from hdfs:

/usr/local/hadoop-1.0.1/bin/hadoop fs -rmr output

To create the hdfs in psuedo-distributed:

bin/hadoop namenode -format

Putting the input on hdfs:

 /usr/local/hadoop-1.0.1/bin/hadoop fs -put test_out/hadoop_input.txt hadoop_input.txt

To start / stop hadoop:

bin/start-all.sh
bin/stop-all.sh


Idea is that the scenario loading for a hadoop run would use any FileDataSink info but
create HadoopDataSinks rather than FileDataSinks. Part of this is updating the 
OutputCollector each time. Can perhaps use the controller code for this.

Debugging in psuedo-distributed mode:

http://localhost:50030/jobtracker.jsp
Click on the job, click on map or reduce, then the task,
then Task Logs