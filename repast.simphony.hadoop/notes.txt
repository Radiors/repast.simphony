Running:

/usr/local/hadoop-1.0.1/bin/hadoop jar ./tmp_jars/repast.simphony.hadoop.jar repast.simphony.engine.BatchJob  -libjars tmp_jars/repast.simphony.batch.jar,tmp_jars/repast.simphony.core.jar,tmp_jars/collections-generic-4.01.jar,tmp_jars/saf.core.runtime.jar,tmp_jars/jscience.jar hadoop_input.txt ./output

Deleting the output from hdfs:

/usr/local/hadoop-1.0.1/bin/hadoop fs -rmr output

To create the hdfs in psuedo-distributed:

bin/hadoop namenode -format

To start / stop hadoop:

bin/start-allsh.
bin/stop-all.sh


Idea is that the scenario loading for a hadoop run would use any FileDataSink info but
create HadoopDataSinks rather than FileDataSinks. Part of this is updating the 
OutputCollector each time. Can perhaps use the controller code for this.